# -*- coding: utf-8 -*-
"""犬種猫種分類器_04-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RCaSCVMR2_JvADJY7s2uSiMX4mnakyD2

# Attention機構（Multiヘッド版）

Attention機構を使うと、モデルの精度を大きく向上させることができる可能性があります。

## 使用するライブラリのimport
"""

import os
import shutil

import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F

from torchvision.models import resnet34
from torch.utils.data import DataLoader

from torchvision.datasets import ImageFolder
import torchvision.transforms as transforms

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# from google.colab import drive
# drive.mount('/content/gdrive')

OUTPUT_DIR = "./"

"""## ユーティリティ関数の定義

データの可視化などに便利な関数をいくつか用意しました。
"""

def tensor_to_image(x):
    x = x * torch.Tensor([0.229, 0.224, 0.225]).reshape(-1, 1, 1)
    x = x + torch.Tensor([0.485, 0.456, 0.406]).reshape(-1, 1, 1)
    return transforms.ToPILImage()(x)

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

def create_confusion_matrix(cm, labels):
    sns.set()
    
    df = pd.DataFrame(cm)
    df.index = labels
    df.columns = labels

    fig, ax = plt.subplots(figsize=(15, 10))
    sns.heatmap(df, annot=True, fmt="d", linewidths=.5, ax=ax, cmap="YlGnBu")

def plot_confusion_matrix(model, dataset):
    images_so_far = 0
    fig = plt.figure()

    targets = np.array([])
    preds = np.array([])

    model.to("cuda")
    model.eval()
    with torch.no_grad():
        for img, target in DataLoader(dataset, shuffle=False, batch_size=64, num_workers=4):
            img = img.to("cuda")
            target = target.to("cuda")
            pred = model(img)
            pred = pred.argmax(dim=1)

            targets = np.append(targets, target.cpu().data.numpy())
            preds = np.append(preds, pred.cpu().numpy())
    
    cm = confusion_matrix(targets, preds)
    create_confusion_matrix(cm, dataset.classes)

"""## データセットの準備

The Oxford-IIIT Pet Datasetのサイトから、データをダウンロードし、展開します。
https://www.robots.ox.ac.uk/~vgg/data/pets/

その後、Train, Validation, Testにデータセットを分割し、

> OxfordPet/{Train|Validation|Test}/{カテゴリ名}/{画像ファイル}
  
というフォルダ構造になるように、ファイルを整理します。
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz -O images.tar.gz -q
# tar -xf images.tar.gz

DATASET_DIR = "dataset"
# if os.path.exists(DATASET_DIR):
#     shutil.rmtree(DATASET_DIR)
#
# categories = []
# for fname in os.listdir("images"):
#     if not fname.endswith(".jpg"):
#         continue
#     elements = os.path.basename(fname).split('_')
#     category = '_'.join(elements[:-1])
#
#     if category not in categories:
#         os.makedirs(os.path.join(DATASET_DIR, "train", category), exist_ok=True)
#         os.makedirs(os.path.join(DATASET_DIR, "valid", category), exist_ok=True)
#         os.makedirs(os.path.join(DATASET_DIR, "test", category), exist_ok=True)
#
#         categories.append(category)
#
# # Validationセット内に存在する誤った画像を除外
# NG_LIST =  ["shiba_inu_156.jpg", "shiba_inu_157.jpg", "saint_bernard_147.jpg", "saint_bernard_148.jpg"]
#
# for category in categories:
#     fnames = []
#     for fname in os.listdir("images"):
#         if not fname.endswith(".jpg") or not fname.startswith(category):
#             continue
#         fnames.append(fname)
#     fnames = sorted(fnames)
#
#     for fname in fnames[:50]:
#         shutil.copy(f"images/{fname}", os.path.join(DATASET_DIR, "Train", category, fname))
#     for fname in fnames[50:100]:
#         if fname in NG_LIST:
#             continue
#         shutil.copy(f"images/{fname}", os.path.join(DATASET_DIR, "Validation", category, fname))
#     for fname in fnames[100:]:
#         shutil.copy(f"images/{fname}", os.path.join(DATASET_DIR, "Test", category, fname))

"""## Step1. データセットの定義

データ拡張を定義し、データを読み込みます。

### データ拡張の定義
"""

augment_transform = transforms.Compose([
    transforms.RandomResizedCrop(size=224),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])
evaluate_transform = transforms.Compose([
    transforms.Resize(size=224),
    transforms.CenterCrop(size=224),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

"""### フォルダからデータセットを作成"""

train_dataset = ImageFolder(f"{DATASET_DIR}/train", transform=augment_transform)
val_dataset = ImageFolder(f"{DATASET_DIR}/valid", transform=evaluate_transform)
test_dataset = ImageFolder(f"{DATASET_DIR}/test", transform=evaluate_transform)

# データセットを可視化してみる
tensor_to_image(train_dataset[0][0])

tensor_to_image(train_dataset[1000][0])

"""## Step3. モデルの定義"""

class MultiAttentionNetwork(nn.Module):
    def __init__(self, num_classes, num_masks=4):
        super().__init__()

        base_model = resnet34(pretrained=True)
        self.features = nn.Sequential(*[layer for layer in base_model.children()][:-2])
        self.attn_conv = nn.Conv2d(512, num_masks, 1, bias=False)
        nn.init.xavier_uniform_(self.attn_conv.weight)
        self.fc = nn.Sequential(
            nn.Linear(512 * num_masks, 256),
            nn.ReLU(True),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes),
        )
        self.mask_ = None
        self.num_masks = num_masks

    def forward(self, x):
        x = self.features(x)
        
        attn = torch.sigmoid(self.attn_conv(x))  # [B, M, H, W]
        B, _, H, W = attn.shape
        self.mask_ = attn
        
        x = x.reshape(B, 1, 512, H, W)
        attn = attn.reshape(B, self.num_masks, 1, H, W)
        
        x = x * attn  # [B, M, 512, H, W]
        x = x.reshape(B * self.num_masks, -1, H, W)  # [BM, 512, H, W]
        x = F.adaptive_avg_pool2d(x, (1, 1))  # [BM, 512, 1, 1]
        
        x = x.reshape(B, -1)
        
        return self.fc(x)
    
    def divergence_loss(self):
        mask = self.mask_  # [B, M, H, W]
        B, M, H, W = mask.shape
        device = mask.device
        
        flatten_mask = mask.reshape(B, M, -1)
        diag = 1 - torch.eye(M).unsqueeze(0).to(device)  # [1, M, M]
        
        max_val, _ = flatten_mask.max(dim=2, keepdim=True)
        flatten_mask = flatten_mask / (max_val + 1e-2)
        
        div_loss = torch.bmm(flatten_mask, flatten_mask.transpose(1, 2)) * diag  # [B, M, M] x [1, M, M]
        return (div_loss.view(-1) ** 2).mean()
        
    
    def save_attention_mask(self, x, path, head=4):
        B = x.shape[0]
        self.forward(x)
        x = x.cpu() * torch.Tensor([0.229, 0.224, 0.225]).reshape(-1, 1, 1)
        x = x + torch.Tensor([0.485, 0.456, 0.406]).reshape(-1, 1, 1)
        fig, axs = plt.subplots(min(B, head), self.num_masks+1, figsize=(16, 2 * min(B, head)), squeeze=False)
        plt.axis('off')
        mask = self.mask_.detach().cpu()
        for i in range(min(B, head)):
            axs[i, 0].imshow(x[i].permute(1, 2, 0))
            for j in range(0, self.num_masks):
                axs[i, j+1].imshow(mask[i, j], vmin = 0, vmax = 1)
        plt.savefig(path)
        plt.close()

"""## Step4. 損失関数の定義"""

clf_loss_func = torch.nn.CrossEntropyLoss()

"""## Step5. 学習/評価の処理

### training関数とevaluation関数の定義
"""

def training(model, dataset, optimizer, lambda_divergence):
    model.train()
  
    correct = 0
    losses = []
    for img, target in DataLoader(dataset, shuffle=True, batch_size=32, num_workers=4):
        img = img.cuda()
        target = target.cuda()
        
        clf = model(img)
        loss = clf_loss_func(clf, target)
        loss += lambda_divergence * model.divergence_loss()

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        losses.append(loss.item())
        
        predict = clf.argmax(dim=1)
        correct += (predict == target).sum().item()
    
    return np.mean(losses), correct / len(dataset)

def evaluation(model, dataset):
    model.eval()
  
    losses = []
    correct = 0
    with torch.no_grad():
        for img, target in DataLoader(dataset, shuffle=False, batch_size=64, num_workers=4):
            img = img.cuda()
            target = target.cuda()
            
            clf = model(img)

            loss = clf_loss_func(clf, target)
            
            losses.append(loss.item())

            predict = clf.argmax(dim=1)
            correct += (predict == target).sum().item()
            
    return np.mean(losses), correct / len(dataset)

num_masks = 4
weight_decay =  1e-5
lambda_divergence = 5e-04

model = MultiAttentionNetwork(len(train_dataset.classes), num_masks=num_masks).to("cuda")

optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=weight_decay)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.9)

output_dir = os.path.join(OUTPUT_DIR, "multi/attentions")
os.makedirs(output_dir, exist_ok=True)

best_loss = 1e+10
best_state = None
earlystop_counter = 0
for epoch in range(100):
    train_loss, train_acc = training(model, train_dataset, optimizer, lambda_divergence)
    val_loss, val_acc = evaluation(model, val_dataset)

    if val_loss < best_loss:
        earlystop_counter = 0
        best_loss = val_loss
        best_state = model.cpu().state_dict()
        model.cuda()
    else:
        earlystop_counter += 1

    if earlystop_counter > 10:
        break

    for img, target in DataLoader(val_dataset, shuffle=True, batch_size=4, num_workers=4):
        img = img.cuda()
        model.save_attention_mask(img, os.path.join(output_dir, f"{str(epoch).zfill(3)}.png"))
        break
    
    scheduler.step()
    
    print(f"epoch {str(epoch).zfill(2)}\ttrain_loss: {train_loss}\ttrain_acc: {train_acc}\tval_loss: {val_loss}\tval_acc: {val_acc}")

model.load_state_dict(best_state)
val_loss, val_acc, *_ = evaluation(model, val_dataset)
print(f"Validationセットに対する分類精度：{val_acc}")
test_loss, test_acc, *_ = evaluation(model, test_dataset)
print(f"Testセットに対する分類精度：{test_acc}")

"""# 評価"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# FILE_ID=1gQcJ6SuZWGTs0zTIA1-zn-yTbSZvXpzs
# FILE_NAME=best_multi.pth
# curl -sc /tmp/cookie "https://drive.google.com/uc?export=download&id=${FILE_ID}" > /dev/null
# CODE="$(awk '/_warning_/ {print $NF}' /tmp/cookie)"  
# curl -Lb /tmp/cookie "https://drive.google.com/uc?export=download&confirm=${CODE}&id=${FILE_ID}" -o ${FILE_NAME}

model = MultiAttentionNetwork(len(train_dataset.classes), num_masks=4).to("cuda")
model.load_state_dict(torch.load("best_multi.pth"))

evaluation(model, val_dataset)

plot_confusion_matrix(model, val_dataset)

evaluation(model, test_dataset)

plot_confusion_matrix(model, test_dataset)

